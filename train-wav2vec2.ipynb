{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e230b0d",
   "metadata": {},
   "source": [
    "When doing prepare dataset following specs are required\n",
    "* 64 GB Ram\n",
    "* 300 GB Disk Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d072e85",
   "metadata": {},
   "source": [
    "When doing training the recomended specs are:\n",
    "* 24GB GPU RAM\n",
    "* 1 GPU (Nvidia GTX 4090 or better)\n",
    "* 300 GB of free disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4acc9a-0a9e-4eca-8dc1-74d48cd4d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#https://towardsdatascience.com/leveraging-the-power-of-jupyter-notebooks-26b4b8d7c622\n",
    "! jupyter notebook --generate-config\n",
    "! jupyter notebook --NotebookApp.max_buffer_size=258000000000\n",
    "! jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df059e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda update -n base -c conda-forge conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install -c pytorch -c conda-forge -c huggingface pandas librosa numpy ipywidgets pytorch torchvision torchaudio datasets transformers wandb huggingface_hub accelerate jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3501bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pandas \n",
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install librosa\n",
    "%pip install wandb -qU\n",
    "%pip install git+https://github.com/huggingface/huggingface_hub\n",
    "%pip install jiwer\n",
    "%pip install transformers[torch]\n",
    "%pip install accelerate -U\n",
    "%pip install ipywidgets\n",
    "%pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2e6a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f6616f593a456c8cb9b5211d8504b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb668a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mspsither_\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ac2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import json\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "import IPython.display as ipd\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9834705f-03b6-43b3-b080-83d0bb13fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2Processor\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset\n",
    "from datasets import ClassLabel\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from jiwer import wer\n",
    "import statistics\n",
    "from transformers import Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir tsv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_train.csv --output tsv/train.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_val.csv --output tsv/validation.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/05_benchmark.csv --output tsv/test.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/vocab.json --output vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2fa73-08da-482b-885b-5fc9300d022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = pd.read_csv(\"tsv/train.csv\")\n",
    "dataValid = pd.read_csv(\"tsv/validation.csv\")\n",
    "dataTest = pd.read_csv(\"tsv/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19daf5-a270-4d0a-aa77-a83a8ed781a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataTrain), len(dataValid), len(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "dataTest['path'] = dataTest['file_name'].apply(lambda x: f'/media/monlamai/SSD/data/wav16k/{x}.wav')\n",
    "\n",
    "dataValid['path'] = dataValid['file_name'].apply(lambda x: f'/media/monlamai/SSD/data/wav16k/{x}.wav')\n",
    "\n",
    "dataTrain['path'] = dataTrain['file_name'].apply(lambda x: f'/media/monlamai/SSD/data/wav16k/{x}.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26bc2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataTest['path'].apply(lambda x: os.path.isfile(x)).value_counts()\n",
    "# dataValid['path'].apply(lambda x: os.path.isfile(x)).value_counts()\n",
    "# batch_df['path'].apply(lambda x: os.path.isfile(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5373f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "common_voice_train = Dataset.from_pandas(dataTrain)\n",
    "common_voice_valid = Dataset.from_pandas(dataValid)\n",
    "common_voice_test = Dataset.from_pandas(dataTest)\n",
    "\n",
    "common_voice_test_transcription = Dataset.from_pandas(dataTest)\n",
    "common_voice_valid_transcription = Dataset.from_pandas(dataValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeec2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(common_voice_train.remove_columns(['dept', 'grade', 'wylie', 'char_len', 'audio_len', 'url']), num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"uni\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defd53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)\n",
    "vocab_valid = common_voice_valid.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_valid.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(\n",
    "    set(vocab_train[\"vocab\"][0]) | \n",
    "    set(vocab_test [\"vocab\"][0]) | \n",
    "    set(vocab_valid[\"vocab\"][0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15663057",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('vocab.json', 'w') as vocab_file:\n",
    "#     json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 cp vocab.json s3://monlam.ai.stt/tsv/vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87386108",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\"./new_vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd0f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4191830",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0157d38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(\"wav2vec2_run10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Resample\n",
    "\n",
    "def speech_file_to_array_fn(batch):\n",
    "    # print(batch)\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    # print(speech_array.shape, sampling_rate)\n",
    "    if sampling_rate != 16000:\n",
    "        print(\"resampling\")\n",
    "        resampler = Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "        speech_array = resampler(speech_array)\n",
    "        sampling_rate = 16000\n",
    "    \n",
    "    # print(speech_array.shape, sampling_rate)\n",
    "    batch[\"speech\"] = speech_array[0].numpy()\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"uni\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(speech_file_to_array_fn, remove_columns=common_voice_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7530260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_voice_valid = common_voice_valid.map(speech_file_to_array_fn, remove_columns=common_voice_valid.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198081c",
   "metadata": {},
   "source": [
    "humm. This does not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int = random.randint(0, len(common_voice_test)-1)\n",
    "\n",
    "ipd.Audio(data=np.asarray(common_voice_test[rand_int][\"path\"]), autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_int = random.randint(0, len(common_voice_test)-1)\n",
    "\n",
    "print(\"Target text:\", common_voice_train[rand_int][\"target_text\"])\n",
    "print(\"Input array shape:\", np.asarray(common_voice_train[rand_int][\"speech\"]).shape)\n",
    "print(\"Sampling rate:\", common_voice_train[rand_int][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"]).input_values\n",
    "    # reshape to (n,)\n",
    "    batch[\"input_values\"] = np.squeeze(batch[\"input_values\"])\n",
    "    # if batch[\"sampling_rate\"] != 16000:\n",
    "    #     print(\"sampling rate not 16k\", batch)\n",
    "    \n",
    "    # with processor.as_target_processor():\n",
    "    #     batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c34b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(batch):\n",
    "#     # check that all files have the correct sampling rate\n",
    "#     assert (\n",
    "#         len(set(batch[\"sampling_rate\"])) == 1\n",
    "#     ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "#     batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "\n",
    "#     with processor.as_target_processor():\n",
    "#         batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "#     return batch\n",
    "\n",
    "# common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, batch_size=4, num_proc=2, batched=True)\n",
    "# common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)\n",
    "# common_voice_valid = common_voice_valid.map(prepare_dataset, remove_columns=common_voice_valid.column_names, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)\n",
    "common_voice_train.save_to_disk(f\"/media/monlamai/SSD/wav2vec2/train_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names)\n",
    "common_voice_test.save_to_disk(\"/media/monlamai/SSD/wav2vec2/test_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30841c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_voice_valid = common_voice_valid.map(prepare_dataset, remove_columns=common_voice_valid.column_names)\n",
    "common_voice_valid.save_to_disk(\"/media/monlamai/SSD/wav2vec2/valid_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770edb87-ad61-4e38-b5b6-48f8c2539aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DatasetDict\n",
    "# ddict = DatasetDict({\n",
    "#     \"train\": common_voice_train,\n",
    "#     \"valid\": common_voice_valid,\n",
    "#     \"test\": common_voice_test,\n",
    "# })\n",
    "# ddict.push_to_hub(\"prepare_dataset_run8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c8e11",
   "metadata": {},
   "source": [
    "### Load the datasets from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "756606dc-4bca-434e-98fd-fc69d1035f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "common_voice_train = load_from_disk('/media/monlamai/SSD/wav2vec2/train_prepare_dataset.arrow')\n",
    "common_voice_test = load_from_disk( '/media/monlamai/SSD/wav2vec2/test_prepare_dataset.arrow')\n",
    "common_voice_valid = load_from_disk('/media/monlamai/SSD/wav2vec2/valid_prepare_dataset.arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204a2c03-1fdd-4f3b-8321-f21824aa16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c740da37-af05-4294-a015-3bd631e2af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2111412b-9d0b-4529-b67f-a7fc747b2665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7451/1679503157.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n",
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/cer/cer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4368847-2256-4b66-a4d7-8c9a7b75718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a8edf07-1f8c-4fb5-b01b-fb09aafa9ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", # commented for for continue training\n",
    "    # \"/media/monlamai/SSD/wav2vec2/wav2vec2_run9/checkpoint-80000\", # inserted for continue training\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, # If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id, # commented for for continue training\n",
    "    vocab_size=len(processor.tokenizer), # commented for for continue training\n",
    "    # ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bb2642e-324c-46a8-bb30-6bb8bb4d0aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1920: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83a07af5-c328-4fcc-8e4e-215fb3080af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.ctc_zero_infinity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab33e6f7-6095-4a5a-a12f-333ce2b9f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"/media/monlamai/SSD/wav2vec2/wav2vec2_run10\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8,\n",
    "  gradient_accumulation_steps=2, # increase by 2x for every 2x decrease in batch size\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=25,\n",
    "  fp16=True,\n",
    "  save_steps=5000,\n",
    "  eval_steps=5000,\n",
    "  logging_steps=100,\n",
    "  report_to=['wandb'],\n",
    "  learning_rate=3e-5,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=10,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd236f00-b36d-46c3-a631-f2507040651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_valid,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b99095de-fbb0-4732-8c67-3a327dee59d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7c7966a68542e1af56fbb812748239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112214855549812, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/monlamai/Documents/GitHub/stt-wav2vec2/wandb/run-20240405_114315-80f8f6jd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/spsither_/huggingface/runs/80f8f6jd/workspace' target=\"_blank\">captain-dukat-45</a></strong> to <a href='https://wandb.ai/spsither_/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/spsither_/huggingface' target=\"_blank\">https://wandb.ai/spsither_/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/spsither_/huggingface/runs/80f8f6jd/workspace' target=\"_blank\">https://wandb.ai/spsither_/huggingface/runs/80f8f6jd/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c191b634573340048811a59a51c51b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1090825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.6289, 'grad_norm': 12.678546905517578, 'learning_rate': 5.82e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4556, 'grad_norm': 7.632822513580322, 'learning_rate': 1.1760000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 5.9109, 'grad_norm': 3.820425271987915, 'learning_rate': 1.776e-05, 'epoch': 0.01}\n",
      "{'loss': 4.1893, 'grad_norm': 2.716172218322754, 'learning_rate': 2.3760000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4893, 'grad_norm': 1.3960139751434326, 'learning_rate': 2.976e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2983, 'grad_norm': 0.9211859703063965, 'learning_rate': 2.9997358585742782e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2572, 'grad_norm': 1.2461479902267456, 'learning_rate': 2.9994607112558185e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2552, 'grad_norm': 1.502617359161377, 'learning_rate': 2.999185563937358e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2228, 'grad_norm': 1.8589173555374146, 'learning_rate': 2.998910416618898e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1686, 'grad_norm': 5.132358551025391, 'learning_rate': 2.998635269300438e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1328, 'grad_norm': 1.1808985471725464, 'learning_rate': 2.9983628734551626e-05, 'epoch': 0.03}\n",
      "{'loss': 3.0556, 'grad_norm': 3.1269874572753906, 'learning_rate': 2.9980877261367025e-05, 'epoch': 0.03}\n",
      "{'loss': 3.0059, 'grad_norm': 1.9309134483337402, 'learning_rate': 2.9978125788182425e-05, 'epoch': 0.03}\n",
      "{'loss': 2.9785, 'grad_norm': 0.914888858795166, 'learning_rate': 2.997537431499782e-05, 'epoch': 0.03}\n",
      "{'loss': 2.9548, 'grad_norm': 1.7705985307693481, 'learning_rate': 2.997262284181322e-05, 'epoch': 0.03}\n",
      "{'loss': 2.9203, 'grad_norm': 2.4505739212036133, 'learning_rate': 2.996987136862862e-05, 'epoch': 0.04}\n",
      "{'loss': 2.8594, 'grad_norm': 1.4735997915267944, 'learning_rate': 2.996711989544402e-05, 'epoch': 0.04}\n",
      "{'loss': 2.7956, 'grad_norm': 1.4692353010177612, 'learning_rate': 2.996436842225942e-05, 'epoch': 0.04}\n",
      "{'loss': 2.6562, 'grad_norm': 1.883301854133606, 'learning_rate': 2.996161694907482e-05, 'epoch': 0.04}\n",
      "{'loss': 2.5448, 'grad_norm': 1.5292388200759888, 'learning_rate': 2.9958865475890218e-05, 'epoch': 0.05}\n",
      "{'loss': 2.4196, 'grad_norm': 2.1487624645233154, 'learning_rate': 2.9956114002705614e-05, 'epoch': 0.05}\n",
      "{'loss': 2.3225, 'grad_norm': 2.2124485969543457, 'learning_rate': 2.9953362529521017e-05, 'epoch': 0.05}\n",
      "{'loss': 2.2351, 'grad_norm': 2.1347708702087402, 'learning_rate': 2.9950611056336413e-05, 'epoch': 0.05}\n",
      "{'loss': 2.1403, 'grad_norm': 1.6870380640029907, 'learning_rate': 2.9947859583151816e-05, 'epoch': 0.06}\n",
      "{'loss': 2.1076, 'grad_norm': 2.3260443210601807, 'learning_rate': 2.9945108109967212e-05, 'epoch': 0.06}\n",
      "{'loss': 2.0848, 'grad_norm': 2.0441744327545166, 'learning_rate': 2.994235663678261e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9957, 'grad_norm': 2.805943727493286, 'learning_rate': 2.993960516359801e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9279, 'grad_norm': 2.2610256671905518, 'learning_rate': 2.9936853690413407e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9114, 'grad_norm': 3.189411163330078, 'learning_rate': 2.993410221722881e-05, 'epoch': 0.07}\n",
      "{'loss': 1.8781, 'grad_norm': 3.509491205215454, 'learning_rate': 2.9931350744044206e-05, 'epoch': 0.07}\n",
      "{'loss': 1.8299, 'grad_norm': 1.660177230834961, 'learning_rate': 2.992859927085961e-05, 'epoch': 0.07}\n",
      "{'loss': 1.8349, 'grad_norm': 3.4582104682922363, 'learning_rate': 2.9925847797675005e-05, 'epoch': 0.07}\n",
      "{'loss': 1.8144, 'grad_norm': 4.335084915161133, 'learning_rate': 2.9923096324490408e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8038, 'grad_norm': 2.7645890712738037, 'learning_rate': 2.9920344851305804e-05, 'epoch': 0.08}\n",
      "{'loss': 1.7725, 'grad_norm': 2.120804786682129, 'learning_rate': 2.99175933781212e-05, 'epoch': 0.08}\n",
      "{'loss': 1.7103, 'grad_norm': 1.8308136463165283, 'learning_rate': 2.9914841904936603e-05, 'epoch': 0.08}\n",
      "{'loss': 1.7426, 'grad_norm': 2.560476064682007, 'learning_rate': 2.9912090431752e-05, 'epoch': 0.08}\n",
      "{'loss': 1.6962, 'grad_norm': 3.319002628326416, 'learning_rate': 2.9909338958567402e-05, 'epoch': 0.09}\n",
      "{'loss': 1.6845, 'grad_norm': 6.122931957244873, 'learning_rate': 2.9906587485382798e-05, 'epoch': 0.09}\n",
      "{'loss': 1.6598, 'grad_norm': 2.5034284591674805, 'learning_rate': 2.99038360121982e-05, 'epoch': 0.09}\n",
      "{'loss': 1.6473, 'grad_norm': 3.348982572555542, 'learning_rate': 2.9901084539013597e-05, 'epoch': 0.09}\n",
      "{'loss': 1.622, 'grad_norm': 2.4442391395568848, 'learning_rate': 2.9898333065828997e-05, 'epoch': 0.1}\n",
      "{'loss': 1.6416, 'grad_norm': 2.806241035461426, 'learning_rate': 2.9895581592644396e-05, 'epoch': 0.1}\n",
      "{'loss': 1.6223, 'grad_norm': 2.6288671493530273, 'learning_rate': 2.9892830119459796e-05, 'epoch': 0.1}\n",
      "{'loss': 1.5995, 'grad_norm': 2.086604595184326, 'learning_rate': 2.9890078646275195e-05, 'epoch': 0.1}\n",
      "{'loss': 1.5938, 'grad_norm': 2.578278064727783, 'learning_rate': 2.988732717309059e-05, 'epoch': 0.11}\n",
      "{'loss': 1.5628, 'grad_norm': 2.3345658779144287, 'learning_rate': 2.988457569990599e-05, 'epoch': 0.11}\n",
      "{'loss': 1.5718, 'grad_norm': 2.5771515369415283, 'learning_rate': 2.988182422672139e-05, 'epoch': 0.11}\n",
      "{'loss': 1.567, 'grad_norm': 4.15703821182251, 'learning_rate': 2.987907275353679e-05, 'epoch': 0.11}\n",
      "{'loss': 1.5447, 'grad_norm': 2.9358103275299072, 'learning_rate': 2.987632128035219e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00f6255272f43c889a1349fb710af84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.217577576637268, 'eval_cer': 0.3819360891059622, 'eval_runtime': 198.9285, 'eval_samples_per_second': 53.446, 'eval_steps_per_second': 6.681, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5522, 'grad_norm': 2.7672829627990723, 'learning_rate': 2.987356980716759e-05, 'epoch': 0.12}\n",
      "{'loss': 1.5291, 'grad_norm': 2.423032283782959, 'learning_rate': 2.987081833398299e-05, 'epoch': 0.12}\n",
      "{'loss': 1.5301, 'grad_norm': 4.001070022583008, 'learning_rate': 2.9868066860798388e-05, 'epoch': 0.12}\n",
      "{'loss': 1.5425, 'grad_norm': 2.237013816833496, 'learning_rate': 2.9865315387613784e-05, 'epoch': 0.12}\n",
      "{'loss': 1.4996, 'grad_norm': 2.425055742263794, 'learning_rate': 2.9862563914429183e-05, 'epoch': 0.13}\n",
      "{'loss': 1.5097, 'grad_norm': 2.681060552597046, 'learning_rate': 2.9859812441244583e-05, 'epoch': 0.13}\n",
      "{'loss': 1.4924, 'grad_norm': 2.6542716026306152, 'learning_rate': 2.9857060968059982e-05, 'epoch': 0.13}\n",
      "{'loss': 1.5164, 'grad_norm': 2.8668181896209717, 'learning_rate': 2.9854309494875382e-05, 'epoch': 0.13}\n",
      "{'loss': 1.485, 'grad_norm': 4.709269046783447, 'learning_rate': 2.985155802169078e-05, 'epoch': 0.14}\n",
      "{'loss': 1.5016, 'grad_norm': 2.4453258514404297, 'learning_rate': 2.984880654850618e-05, 'epoch': 0.14}\n",
      "{'loss': 1.4571, 'grad_norm': 2.315277576446533, 'learning_rate': 2.9846055075321577e-05, 'epoch': 0.14}\n",
      "{'loss': 1.4562, 'grad_norm': 3.2038023471832275, 'learning_rate': 2.984330360213698e-05, 'epoch': 0.14}\n",
      "{'loss': 1.5232, 'grad_norm': 6.7605438232421875, 'learning_rate': 2.9840552128952376e-05, 'epoch': 0.14}\n",
      "{'loss': 1.4915, 'grad_norm': 3.196460247039795, 'learning_rate': 2.983780065576778e-05, 'epoch': 0.15}\n",
      "{'loss': 1.4567, 'grad_norm': 2.657494306564331, 'learning_rate': 2.9835049182583175e-05, 'epoch': 0.15}\n",
      "{'loss': 1.4452, 'grad_norm': 2.7083535194396973, 'learning_rate': 2.9832297709398575e-05, 'epoch': 0.15}\n",
      "{'loss': 1.4432, 'grad_norm': 2.693157911300659, 'learning_rate': 2.9829546236213974e-05, 'epoch': 0.15}\n",
      "{'loss': 1.4439, 'grad_norm': 2.59228253364563, 'learning_rate': 2.982679476302937e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4377, 'grad_norm': 3.5529890060424805, 'learning_rate': 2.9824043289844773e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4333, 'grad_norm': 2.061666965484619, 'learning_rate': 2.982129181666017e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4383, 'grad_norm': 3.648937702178955, 'learning_rate': 2.9818540343475572e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4123, 'grad_norm': 4.557174205780029, 'learning_rate': 2.9815788870290968e-05, 'epoch': 0.17}\n",
      "{'loss': 1.409, 'grad_norm': 3.2900550365448, 'learning_rate': 2.981303739710637e-05, 'epoch': 0.17}\n",
      "{'loss': 1.412, 'grad_norm': 2.674793004989624, 'learning_rate': 2.9810285923921767e-05, 'epoch': 0.17}\n",
      "{'loss': 1.3676, 'grad_norm': 2.4462666511535645, 'learning_rate': 2.9807534450737163e-05, 'epoch': 0.17}\n",
      "{'loss': 1.3901, 'grad_norm': 2.9183223247528076, 'learning_rate': 2.9804782977552566e-05, 'epoch': 0.17}\n",
      "{'loss': 1.4011, 'grad_norm': 3.303074836730957, 'learning_rate': 2.9802031504367962e-05, 'epoch': 0.18}\n",
      "{'loss': 1.396, 'grad_norm': 4.543930530548096, 'learning_rate': 2.9799280031183365e-05, 'epoch': 0.18}\n",
      "{'loss': 1.3928, 'grad_norm': 2.753919839859009, 'learning_rate': 2.979652855799876e-05, 'epoch': 0.18}\n",
      "{'loss': 1.3953, 'grad_norm': 3.2090213298797607, 'learning_rate': 2.9793777084814164e-05, 'epoch': 0.18}\n",
      "{'loss': 1.3884, 'grad_norm': 3.34725022315979, 'learning_rate': 2.979102561162956e-05, 'epoch': 0.19}\n",
      "{'loss': 1.3754, 'grad_norm': 5.178555965423584, 'learning_rate': 2.978827413844496e-05, 'epoch': 0.19}\n",
      "{'loss': 1.393, 'grad_norm': 2.347200632095337, 'learning_rate': 2.978552266526036e-05, 'epoch': 0.19}\n",
      "{'loss': 1.3557, 'grad_norm': 3.560835838317871, 'learning_rate': 2.978277119207576e-05, 'epoch': 0.19}\n",
      "{'loss': 1.3645, 'grad_norm': 2.297689914703369, 'learning_rate': 2.978001971889116e-05, 'epoch': 0.19}\n",
      "{'loss': 1.3588, 'grad_norm': 2.6323928833007812, 'learning_rate': 2.9777268245706554e-05, 'epoch': 0.2}\n",
      "{'loss': 1.3441, 'grad_norm': 2.2879533767700195, 'learning_rate': 2.9774516772521954e-05, 'epoch': 0.2}\n",
      "{'loss': 1.3372, 'grad_norm': 3.4858741760253906, 'learning_rate': 2.9771765299337353e-05, 'epoch': 0.2}\n",
      "{'loss': 1.298, 'grad_norm': 2.932654619216919, 'learning_rate': 2.9769013826152753e-05, 'epoch': 0.2}\n",
      "{'loss': 1.3148, 'grad_norm': 4.634821891784668, 'learning_rate': 2.9766262352968152e-05, 'epoch': 0.21}\n",
      "{'loss': 1.3467, 'grad_norm': 2.580223560333252, 'learning_rate': 2.9763510879783552e-05, 'epoch': 0.21}\n",
      "{'loss': 1.3311, 'grad_norm': 8.929235458374023, 'learning_rate': 2.976075940659895e-05, 'epoch': 0.21}\n",
      "{'loss': 1.3152, 'grad_norm': 5.276480197906494, 'learning_rate': 2.975800793341435e-05, 'epoch': 0.21}\n",
      "{'loss': 1.3071, 'grad_norm': 4.27870512008667, 'learning_rate': 2.9755256460229747e-05, 'epoch': 0.22}\n",
      "{'loss': 1.2959, 'grad_norm': 3.121093511581421, 'learning_rate': 2.9752504987045147e-05, 'epoch': 0.22}\n",
      "{'loss': 1.3057, 'grad_norm': nan, 'learning_rate': 2.9749781028592392e-05, 'epoch': 0.22}\n",
      "{'loss': 1.3164, 'grad_norm': 3.7400596141815186, 'learning_rate': 2.9747029555407792e-05, 'epoch': 0.22}\n",
      "{'loss': 1.3077, 'grad_norm': 3.2364771366119385, 'learning_rate': 2.974427808222319e-05, 'epoch': 0.22}\n",
      "{'loss': 1.329, 'grad_norm': 3.4717814922332764, 'learning_rate': 2.974152660903859e-05, 'epoch': 0.23}\n",
      "{'loss': 1.2991, 'grad_norm': 3.265721321105957, 'learning_rate': 2.973877513585399e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519cec68c9af4e1d905d3f3811159523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9912799000740051, 'eval_cer': 0.3289207219012449, 'eval_runtime': 186.5876, 'eval_samples_per_second': 56.981, 'eval_steps_per_second': 7.123, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3, 'grad_norm': 3.7493510246276855, 'learning_rate': 2.9736023662669386e-05, 'epoch': 0.23}\n",
      "{'loss': 1.263, 'grad_norm': 3.4578096866607666, 'learning_rate': 2.973327218948479e-05, 'epoch': 0.23}\n",
      "{'loss': 1.2788, 'grad_norm': 2.9879515171051025, 'learning_rate': 2.9730520716300185e-05, 'epoch': 0.24}\n",
      "{'loss': 1.2964, 'grad_norm': 2.9814043045043945, 'learning_rate': 2.972776924311559e-05, 'epoch': 0.24}\n",
      "{'loss': 1.2994, 'grad_norm': 3.41569185256958, 'learning_rate': 2.9725017769930984e-05, 'epoch': 0.24}\n",
      "{'loss': 1.3073, 'grad_norm': 3.2432117462158203, 'learning_rate': 2.972226629674638e-05, 'epoch': 0.24}\n",
      "{'loss': 1.2965, 'grad_norm': 2.727620840072632, 'learning_rate': 2.9719514823561783e-05, 'epoch': 0.25}\n",
      "{'loss': 1.287, 'grad_norm': 3.680758476257324, 'learning_rate': 2.971676335037718e-05, 'epoch': 0.25}\n",
      "{'loss': 1.2697, 'grad_norm': 2.932467222213745, 'learning_rate': 2.9714011877192582e-05, 'epoch': 0.25}\n",
      "{'loss': 1.2736, 'grad_norm': 4.38245153427124, 'learning_rate': 2.971126040400798e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3076, 'grad_norm': 4.411777496337891, 'learning_rate': 2.970850893082338e-05, 'epoch': 0.25}\n",
      "{'loss': 1.2883, 'grad_norm': 3.111543655395508, 'learning_rate': 2.9705757457638778e-05, 'epoch': 0.26}\n",
      "{'loss': 1.2858, 'grad_norm': 4.0914387702941895, 'learning_rate': 2.9703005984454177e-05, 'epoch': 0.26}\n",
      "{'loss': 1.2649, 'grad_norm': 2.7101802825927734, 'learning_rate': 2.9700254511269577e-05, 'epoch': 0.26}\n",
      "{'loss': 1.2737, 'grad_norm': 2.4903078079223633, 'learning_rate': 2.9697503038084973e-05, 'epoch': 0.26}\n",
      "{'loss': 1.231, 'grad_norm': 3.099320650100708, 'learning_rate': 2.9694751564900376e-05, 'epoch': 0.27}\n",
      "{'loss': 1.2439, 'grad_norm': 4.184104919433594, 'learning_rate': 2.9692000091715772e-05, 'epoch': 0.27}\n",
      "{'loss': 1.2771, 'grad_norm': 2.969515085220337, 'learning_rate': 2.9689248618531175e-05, 'epoch': 0.27}\n",
      "{'loss': 1.2615, 'grad_norm': 2.23443603515625, 'learning_rate': 2.968649714534657e-05, 'epoch': 0.27}\n",
      "{'loss': 1.276, 'grad_norm': 2.5567030906677246, 'learning_rate': 2.968374567216197e-05, 'epoch': 0.28}\n",
      "{'loss': 1.2371, 'grad_norm': 3.6390318870544434, 'learning_rate': 2.968099419897737e-05, 'epoch': 0.28}\n",
      "{'loss': 1.2578, 'grad_norm': 3.359851121902466, 'learning_rate': 2.967824272579277e-05, 'epoch': 0.28}\n",
      "{'loss': 1.2326, 'grad_norm': 5.976727485656738, 'learning_rate': 2.967549125260817e-05, 'epoch': 0.28}\n",
      "{'loss': 1.2231, 'grad_norm': 2.437716245651245, 'learning_rate': 2.9672739779423568e-05, 'epoch': 0.28}\n",
      "{'loss': 1.239, 'grad_norm': 3.673051118850708, 'learning_rate': 2.9669988306238968e-05, 'epoch': 0.29}\n",
      "{'loss': 1.1925, 'grad_norm': 4.261682033538818, 'learning_rate': 2.9667236833054364e-05, 'epoch': 0.29}\n",
      "{'loss': 1.2168, 'grad_norm': 2.8800575733184814, 'learning_rate': 2.9664485359869763e-05, 'epoch': 0.29}\n",
      "{'loss': 1.2352, 'grad_norm': 3.2568631172180176, 'learning_rate': 2.9661733886685163e-05, 'epoch': 0.29}\n",
      "{'loss': 1.2091, 'grad_norm': 4.487093925476074, 'learning_rate': 2.9658982413500562e-05, 'epoch': 0.3}\n",
      "{'loss': 1.2681, 'grad_norm': 2.9438087940216064, 'learning_rate': 2.9656230940315962e-05, 'epoch': 0.3}\n",
      "{'loss': 1.2286, 'grad_norm': 3.298360824584961, 'learning_rate': 2.965347946713136e-05, 'epoch': 0.3}\n",
      "{'loss': 1.2301, 'grad_norm': 2.6656720638275146, 'learning_rate': 2.965072799394676e-05, 'epoch': 0.3}\n",
      "{'loss': 1.2335, 'grad_norm': 2.441824197769165, 'learning_rate': 2.964797652076216e-05, 'epoch': 0.3}\n",
      "{'loss': 1.2212, 'grad_norm': 3.6167187690734863, 'learning_rate': 2.9645225047577556e-05, 'epoch': 0.31}\n",
      "{'loss': 1.2208, 'grad_norm': 3.1024415493011475, 'learning_rate': 2.9642473574392956e-05, 'epoch': 0.31}\n",
      "{'loss': 1.2215, 'grad_norm': 3.472012519836426, 'learning_rate': 2.9639722101208355e-05, 'epoch': 0.31}\n",
      "{'loss': 1.232, 'grad_norm': nan, 'learning_rate': 2.96369981427556e-05, 'epoch': 0.31}\n",
      "{'loss': 1.2235, 'grad_norm': 3.28656268119812, 'learning_rate': 2.9634246669571e-05, 'epoch': 0.32}\n",
      "{'loss': 1.1944, 'grad_norm': 4.6224894523620605, 'learning_rate': 2.9631495196386397e-05, 'epoch': 0.32}\n",
      "{'loss': 1.2131, 'grad_norm': 3.1404969692230225, 'learning_rate': 2.96287437232018e-05, 'epoch': 0.32}\n",
      "{'loss': 1.2193, 'grad_norm': 5.246313571929932, 'learning_rate': 2.9625992250017196e-05, 'epoch': 0.32}\n",
      "{'loss': 1.2021, 'grad_norm': 3.828638792037964, 'learning_rate': 2.96232407768326e-05, 'epoch': 0.33}\n",
      "{'loss': 1.2008, 'grad_norm': 2.6174724102020264, 'learning_rate': 2.9620489303647995e-05, 'epoch': 0.33}\n",
      "{'loss': 1.1796, 'grad_norm': 4.229582786560059, 'learning_rate': 2.9617737830463398e-05, 'epoch': 0.33}\n",
      "{'loss': 1.1902, 'grad_norm': 3.233097791671753, 'learning_rate': 2.9614986357278794e-05, 'epoch': 0.33}\n",
      "{'loss': 1.201, 'grad_norm': 3.2433743476867676, 'learning_rate': 2.961223488409419e-05, 'epoch': 0.33}\n",
      "{'loss': 1.2193, 'grad_norm': 3.17958927154541, 'learning_rate': 2.9609483410909593e-05, 'epoch': 0.34}\n",
      "{'loss': 1.1821, 'grad_norm': 4.346596717834473, 'learning_rate': 2.960673193772499e-05, 'epoch': 0.34}\n",
      "{'loss': 1.1884, 'grad_norm': 4.4344048500061035, 'learning_rate': 2.9604007979272238e-05, 'epoch': 0.34}\n",
      "{'loss': 1.2106, 'grad_norm': 4.654709815979004, 'learning_rate': 2.9601256506087634e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329daf0098944721bc6cf9f463db1979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8732991218566895, 'eval_cer': 0.29644410030377033, 'eval_runtime': 190.9157, 'eval_samples_per_second': 55.69, 'eval_steps_per_second': 6.961, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1899, 'grad_norm': 4.6852946281433105, 'learning_rate': 2.9598505032903034e-05, 'epoch': 0.35}\n",
      "{'loss': 1.1839, 'grad_norm': 4.08071231842041, 'learning_rate': 2.9595753559718433e-05, 'epoch': 0.35}\n",
      "{'loss': 1.1912, 'grad_norm': 3.5748825073242188, 'learning_rate': 2.9593002086533833e-05, 'epoch': 0.35}\n",
      "{'loss': 1.2008, 'grad_norm': 2.9725000858306885, 'learning_rate': 2.9590250613349232e-05, 'epoch': 0.35}\n",
      "{'loss': 1.2138, 'grad_norm': 4.353885650634766, 'learning_rate': 2.958749914016463e-05, 'epoch': 0.36}\n",
      "{'loss': 1.187, 'grad_norm': 4.606093406677246, 'learning_rate': 2.958474766698003e-05, 'epoch': 0.36}\n",
      "{'loss': 1.1922, 'grad_norm': 3.3849143981933594, 'learning_rate': 2.9581996193795427e-05, 'epoch': 0.36}\n",
      "{'loss': 1.2113, 'grad_norm': 2.885686159133911, 'learning_rate': 2.9579244720610827e-05, 'epoch': 0.36}\n",
      "{'loss': 1.1611, 'grad_norm': 2.76131272315979, 'learning_rate': 2.9576493247426226e-05, 'epoch': 0.36}\n",
      "{'loss': 1.1672, 'grad_norm': 4.152729034423828, 'learning_rate': 2.9573741774241626e-05, 'epoch': 0.37}\n",
      "{'loss': 1.1481, 'grad_norm': 4.028979301452637, 'learning_rate': 2.9570990301057025e-05, 'epoch': 0.37}\n",
      "{'loss': 1.2102, 'grad_norm': 2.825284004211426, 'learning_rate': 2.9568238827872425e-05, 'epoch': 0.37}\n",
      "{'loss': 1.1897, 'grad_norm': 2.9290895462036133, 'learning_rate': 2.9565487354687824e-05, 'epoch': 0.37}\n",
      "{'loss': 1.1655, 'grad_norm': 3.003852367401123, 'learning_rate': 2.9562735881503224e-05, 'epoch': 0.38}\n",
      "{'loss': 1.1966, 'grad_norm': 3.280214548110962, 'learning_rate': 2.955998440831862e-05, 'epoch': 0.38}\n",
      "{'loss': 1.1616, 'grad_norm': 4.480531215667725, 'learning_rate': 2.955723293513402e-05, 'epoch': 0.38}\n",
      "{'loss': 1.1916, 'grad_norm': 3.413914680480957, 'learning_rate': 2.955448146194942e-05, 'epoch': 0.38}\n",
      "{'loss': 1.1509, 'grad_norm': 2.981275796890259, 'learning_rate': 2.955172998876482e-05, 'epoch': 0.39}\n",
      "{'loss': 1.1642, 'grad_norm': 3.2265477180480957, 'learning_rate': 2.9548978515580218e-05, 'epoch': 0.39}\n",
      "{'loss': 1.1513, 'grad_norm': 3.7303521633148193, 'learning_rate': 2.9546227042395618e-05, 'epoch': 0.39}\n",
      "{'loss': 1.2118, 'grad_norm': 3.304250478744507, 'learning_rate': 2.9543475569211017e-05, 'epoch': 0.39}\n",
      "{'loss': 1.1519, 'grad_norm': 3.8566200733184814, 'learning_rate': 2.9540724096026413e-05, 'epoch': 0.39}\n",
      "{'loss': 1.1588, 'grad_norm': 5.402660846710205, 'learning_rate': 2.9537972622841816e-05, 'epoch': 0.4}\n",
      "{'loss': 1.1763, 'grad_norm': 3.7216506004333496, 'learning_rate': 2.9535221149657212e-05, 'epoch': 0.4}\n",
      "{'loss': 1.1495, 'grad_norm': 3.405376434326172, 'learning_rate': 2.953246967647261e-05, 'epoch': 0.4}\n",
      "{'loss': 1.1452, 'grad_norm': 2.4565863609313965, 'learning_rate': 2.952971820328801e-05, 'epoch': 0.4}\n",
      "{'loss': 1.1478, 'grad_norm': 3.825270652770996, 'learning_rate': 2.952696673010341e-05, 'epoch': 0.41}\n",
      "{'loss': 1.1703, 'grad_norm': 3.042635679244995, 'learning_rate': 2.952421525691881e-05, 'epoch': 0.41}\n",
      "{'loss': 1.1637, 'grad_norm': 3.2850112915039062, 'learning_rate': 2.9521463783734206e-05, 'epoch': 0.41}\n",
      "{'loss': 1.1424, 'grad_norm': 4.129653453826904, 'learning_rate': 2.951871231054961e-05, 'epoch': 0.41}\n",
      "{'loss': 1.1412, 'grad_norm': 6.258110046386719, 'learning_rate': 2.9515960837365005e-05, 'epoch': 0.41}\n",
      "{'loss': 1.171, 'grad_norm': 5.471029758453369, 'learning_rate': 2.9513209364180408e-05, 'epoch': 0.42}\n",
      "{'loss': 1.133, 'grad_norm': 3.7577521800994873, 'learning_rate': 2.9510457890995804e-05, 'epoch': 0.42}\n",
      "{'loss': 1.1754, 'grad_norm': 3.9268338680267334, 'learning_rate': 2.9507706417811207e-05, 'epoch': 0.42}\n",
      "{'loss': 1.1309, 'grad_norm': 5.509387016296387, 'learning_rate': 2.9504954944626603e-05, 'epoch': 0.42}\n",
      "{'loss': 1.1142, 'grad_norm': 3.1100013256073, 'learning_rate': 2.9502203471442e-05, 'epoch': 0.43}\n",
      "{'loss': 1.1608, 'grad_norm': 5.692792892456055, 'learning_rate': 2.9499451998257402e-05, 'epoch': 0.43}\n",
      "{'loss': 1.1322, 'grad_norm': 3.307239294052124, 'learning_rate': 2.94967005250728e-05, 'epoch': 0.43}\n",
      "{'loss': 1.1338, 'grad_norm': 5.835635662078857, 'learning_rate': 2.94939490518882e-05, 'epoch': 0.43}\n",
      "{'loss': 1.1523, 'grad_norm': 3.707270383834839, 'learning_rate': 2.9491197578703597e-05, 'epoch': 0.44}\n",
      "{'loss': 1.1394, 'grad_norm': 3.6438071727752686, 'learning_rate': 2.9488446105518997e-05, 'epoch': 0.44}\n",
      "{'loss': 1.1382, 'grad_norm': 3.638437509536743, 'learning_rate': 2.9485694632334396e-05, 'epoch': 0.44}\n",
      "{'loss': 1.1826, 'grad_norm': 3.466276168823242, 'learning_rate': 2.9482943159149796e-05, 'epoch': 0.44}\n",
      "{'loss': 1.1318, 'grad_norm': 2.9637770652770996, 'learning_rate': 2.9480191685965195e-05, 'epoch': 0.44}\n",
      "{'loss': 1.113, 'grad_norm': 3.576364040374756, 'learning_rate': 2.947744021278059e-05, 'epoch': 0.45}\n",
      "{'loss': 1.1093, 'grad_norm': 4.494163990020752, 'learning_rate': 2.9474688739595994e-05, 'epoch': 0.45}\n",
      "{'loss': 1.1092, 'grad_norm': 3.8536152839660645, 'learning_rate': 2.947193726641139e-05, 'epoch': 0.45}\n",
      "{'loss': 1.1382, 'grad_norm': 2.790019989013672, 'learning_rate': 2.946918579322679e-05, 'epoch': 0.45}\n",
      "{'loss': 1.1491, 'grad_norm': 3.3140835762023926, 'learning_rate': 2.946643432004219e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1347, 'grad_norm': 5.547540664672852, 'learning_rate': 2.946368284685759e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f068dffe59004e0c9410a1db59a1b5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8029943108558655, 'eval_cer': 0.279039847519209, 'eval_runtime': 184.0461, 'eval_samples_per_second': 57.768, 'eval_steps_per_second': 7.221, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1028, 'grad_norm': 3.393733501434326, 'learning_rate': 2.946093137367299e-05, 'epoch': 0.46}\n",
      "{'loss': 1.139, 'grad_norm': 3.9251725673675537, 'learning_rate': 2.9458179900488388e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1298, 'grad_norm': 5.944187641143799, 'learning_rate': 2.9455428427303787e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1361, 'grad_norm': 2.8480632305145264, 'learning_rate': 2.9452676954119187e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0867, 'grad_norm': 6.172303199768066, 'learning_rate': 2.9449925480934583e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1105, 'grad_norm': 2.4615957736968994, 'learning_rate': 2.9447174007749983e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0891, 'grad_norm': 10.602849960327148, 'learning_rate': 2.9444422534565382e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0862, 'grad_norm': 4.099671840667725, 'learning_rate': 2.944167106138078e-05, 'epoch': 0.48}\n",
      "{'loss': 1.071, 'grad_norm': 4.153270244598389, 'learning_rate': 2.943891958819618e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1122, 'grad_norm': 5.093905448913574, 'learning_rate': 2.943616811501158e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1009, 'grad_norm': 4.560919761657715, 'learning_rate': 2.9433444156558826e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1049, 'grad_norm': 4.777132987976074, 'learning_rate': 2.9430692683374222e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1241, 'grad_norm': 3.396247148513794, 'learning_rate': 2.9427941210189625e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1465, 'grad_norm': 3.9570817947387695, 'learning_rate': 2.942518973700502e-05, 'epoch': 0.49}\n",
      "{'loss': 1.1098, 'grad_norm': 3.0024399757385254, 'learning_rate': 2.942243826382042e-05, 'epoch': 0.49}\n",
      "{'loss': 1.099, 'grad_norm': 3.4114739894866943, 'learning_rate': 2.941968679063582e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0971, 'grad_norm': 3.474780797958374, 'learning_rate': 2.9416935317451217e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0917, 'grad_norm': 4.2170233726501465, 'learning_rate': 2.941418384426662e-05, 'epoch': 0.5}\n",
      "{'loss': 1.1164, 'grad_norm': 3.0614421367645264, 'learning_rate': 2.9411432371082016e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0857, 'grad_norm': 4.829017639160156, 'learning_rate': 2.940868089789742e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0832, 'grad_norm': 2.494959831237793, 'learning_rate': 2.9405929424712815e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0847, 'grad_norm': 3.2742135524749756, 'learning_rate': 2.9403177951528218e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1301, 'grad_norm': 7.7501702308654785, 'learning_rate': 2.9400426478343614e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0935, 'grad_norm': 4.868256092071533, 'learning_rate': 2.939770251989086e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1073, 'grad_norm': 5.090834617614746, 'learning_rate': 2.939495104670626e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0773, 'grad_norm': 1.7916336059570312, 'learning_rate': 2.9392199573521655e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0897, 'grad_norm': 3.3281209468841553, 'learning_rate': 2.9389448100337058e-05, 'epoch': 0.52}\n",
      "{'loss': 1.049, 'grad_norm': 4.706920623779297, 'learning_rate': 2.9386696627152454e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0525, 'grad_norm': 3.730855941772461, 'learning_rate': 2.9383945153967854e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0915, 'grad_norm': 2.4334096908569336, 'learning_rate': 2.9381193680783253e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1547, 'grad_norm': 3.7547760009765625, 'learning_rate': 2.9378442207598653e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0927, 'grad_norm': 2.9945828914642334, 'learning_rate': 2.9375690734414052e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0797, 'grad_norm': 2.492690324783325, 'learning_rate': 2.937293926122945e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1102, 'grad_norm': 2.7860987186431885, 'learning_rate': 2.937018778804485e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0504, 'grad_norm': 2.217827558517456, 'learning_rate': 2.9367436314860247e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0838, 'grad_norm': 3.014813184738159, 'learning_rate': 2.9364684841675647e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0759, 'grad_norm': 3.2019715309143066, 'learning_rate': 2.9361933368491046e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0531, 'grad_norm': 5.947587966918945, 'learning_rate': 2.9359181895306446e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0769, 'grad_norm': 3.0221240520477295, 'learning_rate': 2.9356430422121845e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0854, 'grad_norm': 3.8687946796417236, 'learning_rate': 2.9353678948937245e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0758, 'grad_norm': 3.1139211654663086, 'learning_rate': 2.9350927475752644e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0796, 'grad_norm': 6.484825134277344, 'learning_rate': 2.9348176002568044e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0696, 'grad_norm': 2.8274950981140137, 'learning_rate': 2.934542452938344e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0732, 'grad_norm': 4.582732677459717, 'learning_rate': 2.9342673056198843e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0751, 'grad_norm': 2.795245409011841, 'learning_rate': 2.933992158301424e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0668, 'grad_norm': 4.344354152679443, 'learning_rate': 2.9337170109829638e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0914, 'grad_norm': 2.7076032161712646, 'learning_rate': 2.9334418636645038e-05, 'epoch': 0.57}\n",
      "{'loss': 1.127, 'grad_norm': 3.078155040740967, 'learning_rate': 2.933169467819228e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0625, 'grad_norm': 3.359308958053589, 'learning_rate': 2.9328943205007683e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0683, 'grad_norm': 3.303891897201538, 'learning_rate': 2.932619173182308e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae245935781b4bc0a0fd308947e15bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7766655087471008, 'eval_cer': 0.2684644707844422, 'eval_runtime': 186.3578, 'eval_samples_per_second': 57.052, 'eval_steps_per_second': 7.131, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.089, 'grad_norm': 2.8128879070281982, 'learning_rate': 2.9323440258638482e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0878, 'grad_norm': 3.7564620971679688, 'learning_rate': 2.9320688785453878e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0675, 'grad_norm': 3.892559289932251, 'learning_rate': 2.931793731226928e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0753, 'grad_norm': 4.933535575866699, 'learning_rate': 2.9315185839084677e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0528, 'grad_norm': 2.541518211364746, 'learning_rate': 2.9312434365900073e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0567, 'grad_norm': 2.8905646800994873, 'learning_rate': 2.9309682892715476e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0723, 'grad_norm': 5.348046779632568, 'learning_rate': 2.9306931419530872e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0501, 'grad_norm': 4.23663330078125, 'learning_rate': 2.9304179946346275e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0437, 'grad_norm': 3.0774238109588623, 'learning_rate': 2.930142847316167e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0907, 'grad_norm': 4.247471332550049, 'learning_rate': 2.9298676999977074e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0321, 'grad_norm': 3.2661190032958984, 'learning_rate': 2.929592552679247e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0854, 'grad_norm': 4.96820068359375, 'learning_rate': 2.929317405360787e-05, 'epoch': 0.6}\n",
      "{'loss': 1.045, 'grad_norm': 3.512882709503174, 'learning_rate': 2.929042258042327e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0525, 'grad_norm': 3.982741117477417, 'learning_rate': 2.928767110723867e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0611, 'grad_norm': 2.7185840606689453, 'learning_rate': 2.9284919634054068e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0847, 'grad_norm': 4.3505940437316895, 'learning_rate': 2.9282168160869464e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0805, 'grad_norm': 2.8089778423309326, 'learning_rate': 2.9279416687684867e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0744, 'grad_norm': 3.1502439975738525, 'learning_rate': 2.9276665214500263e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0491, 'grad_norm': 4.155181884765625, 'learning_rate': 2.9273913741315663e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0701, 'grad_norm': 3.6524155139923096, 'learning_rate': 2.9271162268131062e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0186, 'grad_norm': 4.0447998046875, 'learning_rate': 2.9268410794946462e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0202, 'grad_norm': 4.049670219421387, 'learning_rate': 2.926565932176186e-05, 'epoch': 0.62}\n",
      "{'loss': 1.038, 'grad_norm': 3.548799753189087, 'learning_rate': 2.926290784857726e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0394, 'grad_norm': 8.135008811950684, 'learning_rate': 2.926015637539266e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0622, 'grad_norm': 3.3446226119995117, 'learning_rate': 2.9257404902208057e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0259, 'grad_norm': 4.658166885375977, 'learning_rate': 2.9254653429023456e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0199, 'grad_norm': 3.7071917057037354, 'learning_rate': 2.9251901955838856e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0435, 'grad_norm': 5.251291751861572, 'learning_rate': 2.9249150482654255e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0471, 'grad_norm': 3.032236099243164, 'learning_rate': 2.9246399009469655e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0531, 'grad_norm': 4.992866039276123, 'learning_rate': 2.9243647536285054e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0534, 'grad_norm': 3.0280520915985107, 'learning_rate': 2.9240896063100454e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0605, 'grad_norm': 3.372933864593506, 'learning_rate': 2.9238144589915853e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0392, 'grad_norm': 2.891988515853882, 'learning_rate': 2.923539311673125e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0331, 'grad_norm': 3.6501760482788086, 'learning_rate': 2.9232641643546652e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0257, 'grad_norm': 3.45797061920166, 'learning_rate': 2.9229890170362048e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0233, 'grad_norm': 2.6773507595062256, 'learning_rate': 2.9227138697177448e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0246, 'grad_norm': 2.647153377532959, 'learning_rate': 2.9224387223992847e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0615, 'grad_norm': 4.557353973388672, 'learning_rate': 2.9221635750808243e-05, 'epoch': 0.66}\n",
      "{'loss': 1.057, 'grad_norm': 3.2841506004333496, 'learning_rate': 2.9218884277623646e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0465, 'grad_norm': 3.1987953186035156, 'learning_rate': 2.9216132804439042e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0545, 'grad_norm': 4.462156772613525, 'learning_rate': 2.9213381331254445e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0259, 'grad_norm': 3.181739568710327, 'learning_rate': 2.9210657372801688e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9993, 'grad_norm': 3.513979196548462, 'learning_rate': 2.920790589961709e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0052, 'grad_norm': 3.6037938594818115, 'learning_rate': 2.9205154426432487e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0271, 'grad_norm': 5.087879180908203, 'learning_rate': 2.9202402953247883e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0388, 'grad_norm': 3.5363516807556152, 'learning_rate': 2.9199651480063286e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0778, 'grad_norm': 2.6785924434661865, 'learning_rate': 2.919690000687868e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0295, 'grad_norm': 5.762718200683594, 'learning_rate': 2.9194148533694085e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0467, 'grad_norm': 3.6013994216918945, 'learning_rate': 2.919139706050948e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0136, 'grad_norm': 3.0632002353668213, 'learning_rate': 2.9188645587324884e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aa8b1f02674a82923d9da00117fea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7237769365310669, 'eval_cer': 0.2564059801060218, 'eval_runtime': 188.8521, 'eval_samples_per_second': 56.298, 'eval_steps_per_second': 7.037, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monlamai/Documents/GitHub/stt-wav2vec2/.env/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.02, 'grad_norm': 3.1443629264831543, 'learning_rate': 2.9185921628872126e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0267, 'grad_norm': 3.362224817276001, 'learning_rate': 2.9183170155687525e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0242, 'grad_norm': 3.4033610820770264, 'learning_rate': 2.9180418682502925e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9861, 'grad_norm': 18.24313735961914, 'learning_rate': 2.9177667209318324e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0033, 'grad_norm': 6.177484035491943, 'learning_rate': 2.9174915736133724e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0336, 'grad_norm': 3.3591148853302, 'learning_rate': 2.917216426294912e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0428, 'grad_norm': 3.147200584411621, 'learning_rate': 2.916941278976452e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0116, 'grad_norm': 3.4310007095336914, 'learning_rate': 2.916666131657992e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0341, 'grad_norm': 3.0352351665496826, 'learning_rate': 2.916390984339532e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0194, 'grad_norm': 3.210329055786133, 'learning_rate': 2.9161158370210718e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0164, 'grad_norm': 3.2771623134613037, 'learning_rate': 2.9158406897026118e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0237, 'grad_norm': 4.582292079925537, 'learning_rate': 2.9155655423841517e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0254, 'grad_norm': 3.884748697280884, 'learning_rate': 2.9152903950656917e-05, 'epoch': 0.72}\n"
     ]
    }
   ],
   "source": [
    "# resume_from_checkpoint=True # commented for for continue training\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d783630-2fb6-43ea-8bbe-07a8885f59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\"openpecha/wav2vec2_run9\").to(\"cuda\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"openpecha/wav2vec2_run9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a426cd-511e-4c77-a636-f4f2da728fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = processor(common_voice_test[0][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd7788-3073-4027-945e-9ec13741f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction:\")\n",
    "print(processor.decode(pred_ids))\n",
    "\n",
    "print(\"\\nReference:\")\n",
    "print(common_voice_test_transcription[0][\"sentence\"].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15655ee7-e01f-4fd3-8b03-6fbe896a9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "reference = []\n",
    "paths = []\n",
    "\n",
    "for i in range(0,len(common_voice_test)):\n",
    "\n",
    "  input_dict = processor(common_voice_test[i][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
    "  logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "  pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "  #print(\"Prediction:\")\n",
    "  prediction.append(processor.decode(pred_ids))\n",
    "\n",
    "  #print(\"\\nReference:\")\n",
    "  reference.append(common_voice_test_transcription[i][\"sentence\"].lower())\n",
    "\n",
    "  path = common_voice_test_transcription[i][\"path\"]\n",
    "  path = path.split(\"/\")\n",
    "  path = path[-1]\n",
    "  paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c354322-b869-46dc-812b-9cab7637de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(reference)):\n",
    "  print(paths[i])\n",
    "  print(reference[i])\n",
    "  print(prediction[i])\n",
    "  print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b26b26-93c2-416f-92eb-c5c71e4a91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are necessary for the statistics reporting\n",
    "from google.colab import files\n",
    "import re\n",
    "from jiwer import wer\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c77eff-2065-43d7-ac9a-304c9abf720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Levenshtein Distance between two strings (character distance)\n",
    "# https://colab.research.google.com/github/Alexjmsherman/nlp_practicum_cohort3_instructor/blob/master/lessons/lesson_8_text_similarity/text_similarity_solution.ipynb#scrollTo=sSj3zYpq-sc1\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    # create a matrix\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "\n",
    "    # set col numbers (0, n-1)\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "\n",
    "    # set row numbers (0, n-1)\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    # calculate distance\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            # if characters match do not increase distance\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = matrix[x-1, y-1]\n",
    "            # if characters don't match increase min distance by 1\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c83f76-f498-47ed-a97c-a8908daad63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# Evaluate checkpoints; calculate their word/character error rates and\n",
    "# get the predictions for the sentences in the test set.\n",
    "#===============================================================================\n",
    "\n",
    "checkpointNums = [\"2900\"]\n",
    "\n",
    "medianStats = \"\"\n",
    "\n",
    "for ch in checkpointNums:\n",
    "\n",
    "\tcheckpointNum = ch\n",
    "\n",
    "\tfilename = \"wav2vec2-res-\" + str(runId) + \"-ch\" + ch + \".csv\"\n",
    "\tidThisRun = \"wav2vec2-\" + str(runId)\n",
    "\n",
    "\t# model = Wav2Vec2ForCTC.from_pretrained(\"/content/wav2vec2-large-xlsr/checkpoint-\"+ch).to(\"cuda\")\n",
    "\t# processor = Wav2Vec2Processor.from_pretrained(\"/content/wav2vec2-large-xlsr\")\n",
    "\n",
    "\tinput_dict = processor(common_voice_test[0][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
    "\tlogits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "\tpred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "\tprediction = []\n",
    "\treference = []\n",
    "\tpaths = []\n",
    "\n",
    "\tfor i in range(0,len(common_voice_test)):\n",
    "\n",
    "\t\tinput_dict = processor(common_voice_test[i][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
    "\t\tlogits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "\t\tpred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "\t\t#print(\"Prediction:\")\n",
    "\t\tprediction.append(processor.decode(pred_ids))\n",
    "\n",
    "\t\t#print(\"\\nReference:\")\n",
    "\t\treference.append(common_voice_test_transcription[i][\"sentence\"].lower())\n",
    "\n",
    "\t\tpath = common_voice_test_transcription[i][\"path\"]\n",
    "\t\tpath = path.split(\"/\")\n",
    "\t\tpath = path[-1]\n",
    "\t\tpaths.append(path)\n",
    "\n",
    "\toutput = \"wav,src,res,loss,charDist,charLen,wordDist,wordLen,cer,wer,origin,condition,id,typeMonoTri,ngram\\n\"\n",
    "\tcerList = []\n",
    "\twerList = []\n",
    "\n",
    "\tfor i in range(0,len(reference)):\n",
    "\n",
    "\t\tlevDistChar = levenshtein(reference[i],prediction[i])\n",
    "\t\tcer = levDistChar / len(reference[i])\n",
    "\n",
    "\t\twerSent = wer(reference[i],prediction[i])\n",
    "\t\tcharLen = len(reference[i])\n",
    "\t\tcharDist = levDistChar\n",
    "\t\twordLen = len(prediction[i].split(' '))\n",
    "\t\twordDist = werSent*wordLen\n",
    "\n",
    "\t\tcerList.append(cer)\n",
    "\t\twerList.append(werSent)\n",
    "\n",
    "\t\twavFile = paths[i].replace(\".wav\",\"\")\n",
    "\n",
    "\t\toutput += wavFile + \",\" + reference[i] + \",\" + prediction[i] + \",,\" + str(charDist) + \",\" + str(charLen) + \",\" + str(wordDist) + \",\" + str(wordLen) + \",\" + str(round(cer,2)) + \",\" + str(round(werSent,2)) + \",\" + \"wav2vec2\" + \",\" + \"standard-\" + ch + \",\" + str(idThisRun) + \",\" + \"na\" + \",\" + \"na\" + \"\\n\"\n",
    "\n",
    "\toutput = output[:-1]\n",
    "\t#print(output)\n",
    "\n",
    "\tcerMedian = statistics.median(cerList)\n",
    "\twerMedian = statistics.median(werList)\n",
    "\n",
    "\tmedianStats += runId + \"/\" + ch + \" Median CER:\\t\" + str(round(cerMedian,3)) + \"\\n\"\n",
    "\tmedianStats += runId + \"/\" + ch + \" Median WER:\\t\" + str(round(werMedian,3)) + \"\\n\\n\"\n",
    "\n",
    "\t#print(runId + \"/\" + ch + \" Median CER:\\t\" + str(round(cerMedian,3)))\n",
    "\t#print(runId + \"/\" + ch + \" Median WER:\\t\" + str(round(werMedian,3)))\n",
    "\n",
    "\tprint(output)\n",
    "\n",
    "print(medianStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acafe01-bb7b-477f-93a5-7eddf9b4a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "medianStats = \"Run: \" + runId + \"\\n\\n\" + medianStats\n",
    "\n",
    "statsFilename = \"wav2vec2-res-\"+str(runId)+\"-stats-median.txt\"\n",
    "f = open(datasetPath + \"logs-wav2vec2-res/\" + statsFilename, \"w\")\n",
    "f.write(medianStats)\n",
    "f.close()\n",
    "\n",
    "print(medianStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357f985-cb31-494f-9f01-85ec174abc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of CER and WER\n",
    "df = pd.read_csv(datasetPath + \"logs-wav2vec2-res/\" + filename)\n",
    "df.boxplot(by =['origin'], column =['cer','wer'], grid = False)\n",
    "\n",
    "# For quick visualization only (display only sentences with CER and WER less than 2)\n",
    "dfOnlyLessThanTwo = df[df['cer']<2]\n",
    "dfOnlyLessThanTwo = dfOnlyLessThanTwo[dfOnlyLessThanTwo['wer']<2]\n",
    "dfOnlyLessThanTwo.boxplot(by =['origin'], column =['cer','wer'], grid = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
