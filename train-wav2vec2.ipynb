{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e230b0d",
   "metadata": {},
   "source": [
    "When doing prepare dataset following specs are required\n",
    "* 64 GB Ram\n",
    "* 300 GB Disk Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d072e85",
   "metadata": {},
   "source": [
    "When doing training the recomended specs are:\n",
    "* 24GB GPU RAM\n",
    "* 1 GPU (Nvidia GTX 4090 or better)\n",
    "* 300 GB of free disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4acc9a-0a9e-4eca-8dc1-74d48cd4d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/leveraging-the-power-of-jupyter-notebooks-26b4b8d7c622\n",
    " jupyter notebook --generate-config\n",
    " jupyter notebook --NotebookApp.max_buffer_size=258000000000\n",
    " jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3501bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pandas \n",
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install librosa\n",
    "%pip install wandb -qU\n",
    "%pip install git+https://github.com/huggingface/huggingface_hub\n",
    "%pip install jiwer\n",
    "%pip install transformers[torch]\n",
    "%pip install accelerate -U\n",
    "%pip install ipywidgets\n",
    "%pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad404a35-031c-4d0d-bf51-fe0910d0109b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cc5be-b24f-4244-bddf-309fd6ee2fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm wandb --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e6a38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb668a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb1ba4-2937-40c1-a128-faf97a579ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the WANDB_NOTEBOOK_NAME environment variable\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train-wav2vec2.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b6896-c173-4cc1-9cc4-96001c5d2d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d1f2e3-86f4-4530-8b15-12bc2ad1bb52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"wav2vec2-training\", entity=\"TMLM\", name=\"mms_300_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac2a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import json\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "import IPython.display as ipd\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834705f-03b6-43b3-b080-83d0bb13fb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2Processor\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset\n",
    "from datasets import ClassLabel\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from jiwer import wer\n",
    "import statistics\n",
    "from transformers import Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802efb5-d302-4ce1-b144-c8b69a1dd988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls monlam.ai.stt/tsv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir tsv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_training.csv --output tsv/training.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_validation.csv --output tsv/validation.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/05_benchmarkings.csv --output tsv/test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2fa73-08da-482b-885b-5fc9300d022f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataTrain = pd.read_csv(\"tsv/training.csv\")\n",
    "dataValid = pd.read_csv(\"tsv/validation.csv\")\n",
    "dataTest = pd.read_csv(\"tsv/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19daf5-a270-4d0a-aa77-a83a8ed781a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataTrain), len(dataValid), len(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632258c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 sync s3://monlam.ai.stt/wav16k wav16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "dataTest['path'] = dataTest['file_name'].apply(lambda x: f'/home/ec2-user/Sagemaker/stt-wav2vec2/wav16k/{x}.wav')\n",
    "\n",
    "dataValid['path'] = dataValid['file_name'].apply(lambda x: f'/home/ec2-user/Sagemaker/stt-wav2vec2/wav16k/{x}.wav')\n",
    "\n",
    "dataTrain['path'] = dataTrain['file_name'].apply(lambda x: f'/home/ec2-user/Sagemaker/stt-wav2vec2/wav16k/{x}.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26bc2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataTest['path'].apply(lambda x: os.path.isfile(x)).value_counts()\n",
    "# dataValid['path'].apply(lambda x: os.path.isfile(x)).value_counts()\n",
    "# batch_df['path'].apply(lambda x: os.path.isfile(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5373f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "common_voice_train = Dataset.from_pandas(dataTrain)\n",
    "common_voice_valid = Dataset.from_pandas(dataValid)\n",
    "common_voice_test = Dataset.from_pandas(dataTest)\n",
    "\n",
    "common_voice_test_transcription = Dataset.from_pandas(dataTest)\n",
    "common_voice_valid_transcription = Dataset.from_pandas(dataValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeec2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(common_voice_train.remove_columns(['dept', 'grade', 'wylie', 'char_len', 'audio_len', 'url']), num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"uni\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defd53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)\n",
    "vocab_valid = common_voice_valid.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_valid.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(\n",
    "    set(vocab_train[\"vocab\"][0]) | \n",
    "    set(vocab_test [\"vocab\"][0]) | \n",
    "    set(vocab_valid[\"vocab\"][0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15663057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 cp vocab.json s3://monlam.ai.stt/tsv/vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87386108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0f4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4191830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"mms_300_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Resample\n",
    "\n",
    "def speech_file_to_array_fn(batch):\n",
    "    # print(batch)\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    # print(speech_array.shape, sampling_rate)\n",
    "    if sampling_rate != 16000:\n",
    "        print(\"resampling\")\n",
    "        resampler = Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "        speech_array = resampler(speech_array)\n",
    "        sampling_rate = 16000\n",
    "    \n",
    "    # print(speech_array.shape, sampling_rate)\n",
    "    batch[\"speech\"] = speech_array[0].numpy()\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"uni\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(speech_file_to_array_fn, remove_columns=common_voice_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7530260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_voice_valid = common_voice_valid.map(speech_file_to_array_fn, remove_columns=common_voice_valid.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198081c",
   "metadata": {},
   "source": [
    "humm. This does not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int = random.randint(0, len(common_voice_test)-1)\n",
    "\n",
    "ipd.Audio(data=np.asarray(common_voice_test[rand_int][\"path\"]), autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_int = random.randint(0, len(common_voice_test)-1)\n",
    "\n",
    "print(\"Target text:\", common_voice_train[rand_int][\"target_text\"])\n",
    "print(\"Input array shape:\", np.asarray(common_voice_train[rand_int][\"speech\"]).shape)\n",
    "print(\"Sampling rate:\", common_voice_train[rand_int][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"]).input_values\n",
    "    # reshape to (n,)\n",
    "    batch[\"input_values\"] = np.squeeze(batch[\"input_values\"])\n",
    "    # if batch[\"sampling_rate\"] != 16000:\n",
    "    #     print(\"sampling rate not 16k\", batch)\n",
    "    \n",
    "    # with processor.as_target_processor():\n",
    "    #     batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c34b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(batch):\n",
    "#     # check that all files have the correct sampling rate\n",
    "#     assert (\n",
    "#         len(set(batch[\"sampling_rate\"])) == 1\n",
    "#     ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "#     batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "\n",
    "#     with processor.as_target_processor():\n",
    "#         batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "#     return batch\n",
    "\n",
    "# common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, batch_size=4, num_proc=2, batched=True)\n",
    "# common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)\n",
    "# common_voice_valid = common_voice_valid.map(prepare_dataset, remove_columns=common_voice_valid.column_names, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)\n",
    "common_voice_train.save_to_disk(f\"/media/monlamai/SSD/wav2vec2/train_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names)\n",
    "common_voice_test.save_to_disk(\"/media/monlamai/SSD/wav2vec2/test_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30841c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_voice_valid = common_voice_valid.map(prepare_dataset, remove_columns=common_voice_valid.column_names)\n",
    "common_voice_valid.save_to_disk(\"/media/monlamai/SSD/wav2vec2/valid_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770edb87-ad61-4e38-b5b6-48f8c2539aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DatasetDict\n",
    "# ddict = DatasetDict({\n",
    "#     \"train\": common_voice_train,\n",
    "#     \"valid\": common_voice_valid,\n",
    "#     \"test\": common_voice_test,\n",
    "# })\n",
    "# ddict.push_to_hub(\"prepare_dataset_run8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c8e11",
   "metadata": {},
   "source": [
    "### Load the datasets from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc09fe-e4e1-44a7-adfb-68ec804f7f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r wav16k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57be7f1-d856-4c25-aced-9b1aef946a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 cp s3://monlam.ai.stt/wav16k/STT_MV0833_0298_2228254_to_2237817.wav STT_MV0833_0298_2228254_to_2237817.wav\n",
    "! aws s3 cp s3://monlam.ai.stt/wav16k/STT_MV0833_0291_2194075_to_2195733.wav STT_MV0833_0291_2194075_to_2195733.wav\n",
    "! aws s3 cp s3://monlam.ai.stt/wav16k/STT_MV0833_0293_2205438_to_2212722.wav STT_MV0833_0293_2205438_to_2212722.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356386d3-e937-4fa0-830d-5a1320a4871d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 ls s3://monlam.ai.stt/dataset/wav2vec2/train_prepare_dataset.arrow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756606dc-4bca-434e-98fd-fc69d1035f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "common_voice_train = load_from_disk('/home/ec2-user/SageMaker/stt-wav2vec2/data/train_prepare_dataset.arrow')\n",
    "common_voice_test =  load_from_disk( '/home/ec2-user/SageMaker/stt-wav2vec2/data/test_prepare_dataset.arrow')\n",
    "common_voice_valid = load_from_disk('/home/ec2-user/SageMaker/stt-wav2vec2/data/valid_prepare_dataset.arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b03670-2c89-4d2a-94de-279f9215a19b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_voice_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d785769-d474-4a98-8f7c-de61d29f5be9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_voice_train[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dab1d5-974b-4359-a94e-e33e25c6dad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f72524-a128-4268-8701-afc984b86054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained('mms_300_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef92adc-f08f-4a0e-922a-ce75d584dde0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d1f8f-221a-43ae-ad84-97b6763596fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor.tokenizer.decode(common_voice_train[89]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4238d-ec3b-426d-90b3-0070e1c401e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_voice_train[0]['input_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a2c03-1fdd-4f3b-8321-f21824aa16d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740da37-af05-4294-a015-3bd631e2af37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111412b-9d0b-4529-b67f-a7fc747b2665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4368847-2256-4b66-a4d7-8c9a7b75718d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8edf07-1f8c-4fb5-b01b-fb09aafa9ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    # \"/media/monlamai/SSD/mms_300/mms_300_v1/checkpoint-1140000\", # inserted for continue training\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    # gradient_checkpointing=True, # If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id, # commented for for continue training\n",
    "    vocab_size=len(processor.tokenizer), # commented for for continue training\n",
    "    # ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2642e-324c-46a8-bb30-6bb8bb4d0aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a07af5-c328-4fcc-8e4e-215fb3080af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.ctc_zero_infinity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33e6f7-6095-4a5a-a12f-333ce2b9f9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./result\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=8, # \n",
    "  gradient_accumulation_steps=2, # increase by 2x for every 2x decrease in batch size\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=10000,\n",
    "  eval_steps=10000,\n",
    "  logging_steps=20,\n",
    "  report_to=['wandb'],\n",
    "  learning_rate=3e-5,\n",
    "  warmup_steps=50,\n",
    "  save_total_limit=40,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd236f00-b36d-46c3-a631-f2507040651e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_valid,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99095de-fbb0-4732-8c67-3a327dee59d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.train(resume_from_checkpoint=True)  # uncomment for continue training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f43990-1abb-4713-9b15-e561471c88de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d783630-2fb6-43ea-8bbe-07a8885f59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\"openpecha/wav2vec2_run10\").to(\"cuda\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"openpecha/wav2vec2_run10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a426cd-511e-4c77-a636-f4f2da728fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = processor(common_voice_test[0][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd7788-3073-4027-945e-9ec13741f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction:\")\n",
    "print(processor.decode(pred_ids))\n",
    "\n",
    "print(\"\\nReference:\")\n",
    "print(common_voice_test_transcription[0][\"sentence\"].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15655ee7-e01f-4fd3-8b03-6fbe896a9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "reference = []\n",
    "paths = []\n",
    "\n",
    "for i in range(0,len(common_voice_test)):\n",
    "\n",
    "  input_dict = processor(common_voice_test[i][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
    "  logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "  pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "  #print(\"Prediction:\")\n",
    "  prediction.append(processor.decode(pred_ids))\n",
    "\n",
    "  #print(\"\\nReference:\")\n",
    "  reference.append(common_voice_test_transcription[i][\"sentence\"].lower())\n",
    "\n",
    "  path = common_voice_test_transcription[i][\"path\"]\n",
    "  path = path.split(\"/\")\n",
    "  path = path[-1]\n",
    "  paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c354322-b869-46dc-812b-9cab7637de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(reference)):\n",
    "  print(paths[i])\n",
    "  print(reference[i])\n",
    "  print(prediction[i])\n",
    "  print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b26b26-93c2-416f-92eb-c5c71e4a91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are necessary for the statistics reporting\n",
    "from google.colab import files\n",
    "import re\n",
    "from jiwer import wer\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c77eff-2065-43d7-ac9a-304c9abf720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Levenshtein Distance between two strings (character distance)\n",
    "# https://colab.research.google.com/github/Alexjmsherman/nlp_practicum_cohort3_instructor/blob/master/lessons/lesson_8_text_similarity/text_similarity_solution.ipynb#scrollTo=sSj3zYpq-sc1\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    # create a matrix\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "\n",
    "    # set col numbers (0, n-1)\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "\n",
    "    # set row numbers (0, n-1)\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    # calculate distance\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            # if characters match do not increase distance\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = matrix[x-1, y-1]\n",
    "            # if characters don't match increase min distance by 1\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c83f76-f498-47ed-a97c-a8908daad63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# Evaluate checkpoints; calculate their word/character error rates and\n",
    "# get the predictions for the sentences in the test set.\n",
    "#===============================================================================\n",
    "\n",
    "checkpointNums = [\"2900\"]\n",
    "\n",
    "medianStats = \"\"\n",
    "\n",
    "for ch in checkpointNums:\n",
    "\n",
    "\tcheckpointNum = ch\n",
    "\n",
    "\tfilename = \"wav2vec2-res-\" + str(runId) + \"-ch\" + ch + \".csv\"\n",
    "\tidThisRun = \"wav2vec2-\" + str(runId)\n",
    "\n",
    "\t# model = Wav2Vec2ForCTC.from_pretrained(\"/content/wav2vec2-large-xlsr/checkpoint-\"+ch).to(\"cuda\")\n",
    "\t# processor = Wav2Vec2Processor.from_pretrained(\"/content/wav2vec2-large-xlsr\")\n",
    "\n",
    "\tinput_dict = processor(common_voice_test[0][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
    "\tlogits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "\tpred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "\tprediction = []\n",
    "\treference = []\n",
    "\tpaths = []\n",
    "\n",
    "\tfor i in range(0,len(common_voice_test)):\n",
    "\n",
    "\t\tinput_dict = processor(common_voice_test[i][\"input_values\"], return_tensors=\"pt\", padding=True)\n",
    "\t\tlogits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "\t\tpred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "\t\t#print(\"Prediction:\")\n",
    "\t\tprediction.append(processor.decode(pred_ids))\n",
    "\n",
    "\t\t#print(\"\\nReference:\")\n",
    "\t\treference.append(common_voice_test_transcription[i][\"sentence\"].lower())\n",
    "\n",
    "\t\tpath = common_voice_test_transcription[i][\"path\"]\n",
    "\t\tpath = path.split(\"/\")\n",
    "\t\tpath = path[-1]\n",
    "\t\tpaths.append(path)\n",
    "\n",
    "\toutput = \"wav,src,res,loss,charDist,charLen,wordDist,wordLen,cer,wer,origin,condition,id,typeMonoTri,ngram\\n\"\n",
    "\tcerList = []\n",
    "\twerList = []\n",
    "\n",
    "\tfor i in range(0,len(reference)):\n",
    "\n",
    "\t\tlevDistChar = levenshtein(reference[i],prediction[i])\n",
    "\t\tcer = levDistChar / len(reference[i])\n",
    "\n",
    "\t\twerSent = wer(reference[i],prediction[i])\n",
    "\t\tcharLen = len(reference[i])\n",
    "\t\tcharDist = levDistChar\n",
    "\t\twordLen = len(prediction[i].split(' '))\n",
    "\t\twordDist = werSent*wordLen\n",
    "\n",
    "\t\tcerList.append(cer)\n",
    "\t\twerList.append(werSent)\n",
    "\n",
    "\t\twavFile = paths[i].replace(\".wav\",\"\")\n",
    "\n",
    "\t\toutput += wavFile + \",\" + reference[i] + \",\" + prediction[i] + \",,\" + str(charDist) + \",\" + str(charLen) + \",\" + str(wordDist) + \",\" + str(wordLen) + \",\" + str(round(cer,2)) + \",\" + str(round(werSent,2)) + \",\" + \"wav2vec2\" + \",\" + \"standard-\" + ch + \",\" + str(idThisRun) + \",\" + \"na\" + \",\" + \"na\" + \"\\n\"\n",
    "\n",
    "\toutput = output[:-1]\n",
    "\t#print(output)\n",
    "\n",
    "\tcerMedian = statistics.median(cerList)\n",
    "\twerMedian = statistics.median(werList)\n",
    "\n",
    "\tmedianStats += runId + \"/\" + ch + \" Median CER:\\t\" + str(round(cerMedian,3)) + \"\\n\"\n",
    "\tmedianStats += runId + \"/\" + ch + \" Median WER:\\t\" + str(round(werMedian,3)) + \"\\n\\n\"\n",
    "\n",
    "\t#print(runId + \"/\" + ch + \" Median CER:\\t\" + str(round(cerMedian,3)))\n",
    "\t#print(runId + \"/\" + ch + \" Median WER:\\t\" + str(round(werMedian,3)))\n",
    "\n",
    "\tprint(output)\n",
    "\n",
    "print(medianStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acafe01-bb7b-477f-93a5-7eddf9b4a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "medianStats = \"Run: \" + runId + \"\\n\\n\" + medianStats\n",
    "\n",
    "statsFilename = \"wav2vec2-res-\"+str(runId)+\"-stats-median.txt\"\n",
    "f = open(datasetPath + \"logs-wav2vec2-res/\" + statsFilename, \"w\")\n",
    "f.write(medianStats)\n",
    "f.close()\n",
    "\n",
    "print(medianStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357f985-cb31-494f-9f01-85ec174abc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of CER and WER\n",
    "df = pd.read_csv(datasetPath + \"logs-wav2vec2-res/\" + filename)\n",
    "df.boxplot(by =['origin'], column =['cer','wer'], grid = False)\n",
    "\n",
    "# For quick visualization only (display only sentences with CER and WER less than 2)\n",
    "dfOnlyLessThanTwo = df[df['cer']<2]\n",
    "dfOnlyLessThanTwo = dfOnlyLessThanTwo[dfOnlyLessThanTwo['wer']<2]\n",
    "dfOnlyLessThanTwo.boxplot(by =['origin'], column =['cer','wer'], grid = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
