{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 ls s3://monlam.ai.stt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 sync s3://monlam.ai.stt/wav16k wav16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir tsv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_training.csv --output tsv/training.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_validation.csv --output tsv/validation.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/05_benchmarkings.csv --output tsv/test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataTrain = pd.read_csv(\"tsv/training.csv\")\n",
    "dataValid = pd.read_csv(\"tsv/validation.csv\")\n",
    "dataTest = pd.read_csv(\"tsv/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dataTrain), len(dataValid), len(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataTrain = dataTrain[dataTrain['file_name'] != 'STT_AB00321_1248_4868796_to_4870964']\n",
    "dataTest = dataTest[dataTest['file_name'] != 'STT_MV0246_0343_2208363_to_2216623']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "dataTest['path'] = dataTest['file_name'].apply(lambda x: f'wav16k/{x}.wav')\n",
    "dataValid['path'] = dataValid['file_name'].apply(lambda x: f'wav16k/{x}.wav')\n",
    "dataTrain['path'] = dataTrain['file_name'].apply(lambda x: f'wav16k/{x}.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"mms_300_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='error_log.log',\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
    ")\n",
    "\n",
    "# Assuming 'processor' is predefined, e.g., from Hugging Face's transformers library\n",
    "def prepare_dataset(batch):\n",
    "    try:\n",
    "        if \"speech\" not in batch or len(batch[\"speech\"]) == 0:\n",
    "            error_message = f\"Empty speech data in batch: {batch}\"\n",
    "            print(error_message)\n",
    "            logging.error(error_message)\n",
    "            batch[\"input_values\"] = np.array([], dtype=np.float32)  # Set default empty array with consistent type\n",
    "            batch[\"labels\"] = []  # Set default empty list\n",
    "            batch[\"valid\"] = False\n",
    "            return batch  # Indicate that this row should be discarded\n",
    "\n",
    "        batch[\"speech\"] = np.array(batch[\"speech\"], dtype=np.float32)  # Ensure speech is a NumPy array with consistent type\n",
    "\n",
    "        batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"]).input_values\n",
    "        # Reshape to (n,)\n",
    "        batch[\"input_values\"] = np.squeeze(batch[\"input_values\"])\n",
    "\n",
    "        if \"target_text\" not in batch or batch[\"target_text\"] == \"\":\n",
    "            error_message = f\"Empty target text in batch: {batch}\"\n",
    "            print(error_message)\n",
    "            logging.error(error_message)\n",
    "            batch[\"input_values\"] = np.array([], dtype=np.float32)  # Set default empty array with consistent type\n",
    "            batch[\"labels\"] = []  # Set default empty list\n",
    "            batch[\"valid\"] = False\n",
    "            return batch  # Indicate that this row should be discarded\n",
    "\n",
    "        batch[\"labels\"] = processor(text=batch[\"target_text\"]).input_ids\n",
    "        batch[\"valid\"] = True  # Indicate that this row is valid\n",
    "        return batch\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error in prepare_dataset: {e}\"\n",
    "        print(error_message)\n",
    "        logging.error(error_message)\n",
    "        batch[\"input_values\"] = np.array([], dtype=np.float32)  # Set default empty array with consistent type\n",
    "        batch[\"labels\"] = []  # Set default empty list\n",
    "        batch[\"valid\"] = False\n",
    "        return batch  # Indicate that this row should be discarded\n",
    "\n",
    "def speech_file_to_array_fn(batch):\n",
    "    try:\n",
    "        if not os.path.exists(batch[\"path\"]):\n",
    "            raise OSError(f\"File not found: {batch['path']}\")\n",
    "\n",
    "        speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "       \n",
    "        if sampling_rate != 16000:\n",
    "            print(\"Resampling\")\n",
    "            resampler = Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "            speech_array = resampler(speech_array)\n",
    "            sampling_rate = 16000\n",
    "            \n",
    "        if len(speech_array) == 0:\n",
    "            raise ValueError(f\"Empty speech data in file: {batch['path']}\")\n",
    "\n",
    "        batch[\"speech\"] = speech_array[0].numpy().astype(np.float32)  # Ensure consistent type\n",
    "        batch[\"sampling_rate\"] = sampling_rate\n",
    "        batch[\"target_text\"] = batch[\"uni\"]\n",
    "        batch[\"valid\"] = True\n",
    "        return batch  # Indicate that this row is valid\n",
    "    except (OSError, ValueError, Exception) as e:\n",
    "        error_message = f\"Error processing audio file {batch['path']}: {e}\"\n",
    "        print(error_message)\n",
    "        logging.error(error_message)\n",
    "        batch[\"speech\"] = np.array([], dtype=np.float32)  # Set default empty array with consistent type\n",
    "        batch[\"sampling_rate\"] = 16000\n",
    "        batch[\"target_text\"] = \"\"\n",
    "        batch[\"valid\"] = False\n",
    "        return batch  # Indicate that this row should be discarded\n",
    "\n",
    "def process_batch(batch_i, batch_df):\n",
    "    try:\n",
    "        print(f\"Processing speech batch {batch_i}\")\n",
    "        common_voice_train = Dataset.from_pandas(batch_df)\n",
    "        \n",
    "        # Process the dataset\n",
    "        common_voice_train = common_voice_train.map(\n",
    "            speech_file_to_array_fn, \n",
    "            batched=False\n",
    "        )\n",
    "        print(f\"Filtering invalid speech rows {batch_i}\")\n",
    "        common_voice_train = common_voice_train.filter(lambda x: x['valid'])\n",
    "        # Remove 'valid' column after filtering\n",
    "        print(f\"Removing valid column {batch_i}\")\n",
    "        common_voice_train = common_voice_train.remove_columns(['valid'])\n",
    "        \n",
    "        \n",
    "        print(f\"Processing prepare_dataset batch {batch_i}\")\n",
    "\n",
    "        common_voice_train = common_voice_train.map(\n",
    "            prepare_dataset, \n",
    "            batched=False\n",
    "        )\n",
    "        print(f\"Filtering invalid dataset rows {batch_i}\")\n",
    "        common_voice_train = common_voice_train.filter(lambda x: x['valid'])\n",
    "        \n",
    "        # Remove 'valid' column after filtering\n",
    "        print(f\"Removing valid column {batch_i}\")\n",
    "        common_voice_train = common_voice_train.remove_columns(['valid'])\n",
    "        \n",
    "        # Save the processed batch to disk\n",
    "        common_voice_train.save_to_disk(f\"data/train_prepare_dataset_batch_{batch_i}.arrow\")\n",
    "        print(f\"Saved processed batch {batch_i} to disk.\")\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing batch {batch_i}: {e}\"\n",
    "        print(error_message)\n",
    "        logging.error(error_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "total = len(dataTrain)\n",
    "batch_size = math.floor(total * 2/100)\n",
    "\n",
    "max_batch_i = math.floor(total/batch_size) - 1\n",
    "\n",
    "print(f'total: {total}, batch_size: {batch_size}, max_batch_i: {max_batch_i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your data into a pandas DataFrame\n",
    "\n",
    "for batch_i in range(0, max_batch_i+1):\n",
    "\n",
    "    batch_df = dataTrain[batch_i * batch_size:] if batch_i == max_batch_i else dataTrain[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "\n",
    "    # Save the batch DataFrame to a CSV file\n",
    "    batch_df.to_csv(f\"batch_data/batch_{batch_i}.csv\", index=False)\n",
    "    print(\"saved\", batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multiprocessing\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = []\n",
    "\n",
    "for batch_i in range(0, max_batch_i + 1):\n",
    "    if os.path.exists(f\"data/train_prepare_dataset_batch_{batch_i}.arrow\"):\n",
    "        continue\n",
    "    batch_df = pd.read_csv(f\"batch_data/batch_{batch_i}.csv\")\n",
    "    batch_df = batch_df[~batch_df['file_name'].str.startswith('STT_MV0833')]\n",
    "    result = pool.apply_async(process_batch, args=(batch_i, batch_df))\n",
    "    results.append(result)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Ensure all processes are completed\n",
    "for result in results:\n",
    "    result.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "common_voice_valid = Dataset.from_pandas(dataValid)\n",
    "common_voice_test = Dataset.from_pandas(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_voice_valid = common_voice_valid.map(speech_file_to_array_fn, remove_columns=common_voice_valid.column_names)\n",
    "common_voice_valid = common_voice_valid.map(prepare_dataset, remove_columns=common_voice_valid.column_names)\n",
    "common_voice_valid.save_to_disk(\"data/valid_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)\n",
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names)\n",
    "common_voice_test.save_to_disk(\"data/test_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "train_arr = []\n",
    "\n",
    "for i in range(max_batch_i + 1):\n",
    "    print(i)\n",
    "    train_batch_i = load_from_disk(f'data/train_prepare_dataset_batch_{i}.arrow')\n",
    "    train_arr.append(train_batch_i)\n",
    "\n",
    "common_voice_train = concatenate_datasets(train_arr)\n",
    "\n",
    "# common_voice_train.save_to_disk(\"/media/monlamai/HD_volume_1/wav2vec2/train_prepare_dataset.arrow\")\n",
    "common_voice_train.save_to_disk(\"data/train_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp data/valid_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/valid_prepare_dataset.arrow --recursive\n",
    "!aws s3 cp data/test_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/test_prepare_dataset.arrow --recursive\n",
    "!aws s3 cp data/train_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/train_prepare_dataset.arrow--recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync data/valid_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/valid_prepare_dataset.arrow \n",
    "!aws s3 sync data/test_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/test_prepare_dataset.arrow \n",
    "!aws s3 sync data/train_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/train_prepare_dataset.arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mv HD_volume_1/wav2vec2/train_prepare_dataset.arrow SSD/wav2vec2\n",
    "# aws s3 cp valid_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/valid_prepare_dataset.arrow --recursive\n",
    "# aws s3 cp test_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/test_prepare_dataset.arrow --recursive\n",
    "# aws s3 cp train_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/train_prepare_dataset.arrow --recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"mms_300_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push best model to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm data/train_prepare_dataset_batch_*.arrow --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"/home/ec2-user/Sagemaker/stt-wav2vec2/mms_300/mms_300_v1/checkpoint-1190000\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mms_300_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mms_300_v2.1190\"\n",
    "model.push_to_hub(    model_name)\n",
    "processor.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
