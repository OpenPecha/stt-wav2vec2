{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 sync s3://monlam.ai.stt/wav16k wav16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "common_voice_test =  load_from_disk( '/media/monlamai/SSD/wav2vec2/test_prepare_dataset.arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir tsv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_train.csv --output tsv/train.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_val.csv --output tsv/validation.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/05_benchmark.csv --output tsv/test.csv\n",
    "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/vocab.json --output vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataTrain = pd.read_csv(\"tsv/train.csv\")\n",
    "dataValid = pd.read_csv(\"tsv/validation.csv\")\n",
    "dataTest = pd.read_csv(\"tsv/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = dataTrain[dataTrain['file_name'] != 'STT_AB00321_1248_4868796_to_4870964']\n",
    "dataTest = dataTest[dataTest['file_name'] != 'STT_MV0246_0343_2208363_to_2216623']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "dataTest['path'] = dataTest['file_name'].apply(lambda x: f'/media/monlamai/SSD/data/wav16k/{x}.wav')\n",
    "dataValid['path'] = dataValid['file_name'].apply(lambda x: f'/media/monlamai/SSD/data/wav16k/{x}.wav')\n",
    "dataTrain['path'] = dataTrain['file_name'].apply(lambda x: f'/media/monlamai/SSD/data/wav16k/{x}.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Resample\n",
    "import torchaudio\n",
    "\n",
    "def speech_file_to_array_fn(batch):\n",
    "    # print(batch)\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    # print(speech_array.shape, sampling_rate)\n",
    "    if sampling_rate != 16000:\n",
    "        print(\"resampling\")\n",
    "        resampler = Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "        speech_array = resampler(speech_array)\n",
    "        sampling_rate = 16000\n",
    "    \n",
    "    # print(speech_array.shape, sampling_rate)\n",
    "    batch[\"speech\"] = speech_array[0].numpy()\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"uni\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"]).input_values\n",
    "    # reshape to (n,)\n",
    "    batch[\"input_values\"] = np.squeeze(batch[\"input_values\"])\n",
    "    # if batch[\"sampling_rate\"] != 16000:\n",
    "    #     print(\"sampling rate not 16k\", batch)\n",
    "    \n",
    "    # with processor.as_target_processor():\n",
    "    #     batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "total = len(dataTrain)\n",
    "batch_size = math.floor(total * 5/100)\n",
    "\n",
    "max_batch_i = math.floor(total/batch_size) - 1\n",
    "\n",
    "print(f'total: {total}, batch_size: {batch_size}, max_batch_i: {max_batch_i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "common_voice_valid = Dataset.from_pandas(dataValid)\n",
    "common_voice_test = Dataset.from_pandas(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i in range(0, max_batch_i+1):\n",
    "    print('batch_i', batch_i)\n",
    "\n",
    "    batch_df = dataTrain[batch_i*batch_size:] if batch_i == max_batch_i else dataTrain[batch_i*batch_size:(batch_i+1)*batch_size]\n",
    "\n",
    "    common_voice_train = Dataset.from_pandas(batch_df)\n",
    "    common_voice_train = common_voice_train.map(speech_file_to_array_fn, remove_columns=common_voice_train.column_names)\n",
    "    common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)\n",
    "    common_voice_train.save_to_disk(f\"/media/monlamai/SSD/wav2vec2/train_prepare_dataset_batch_{batch_i}.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_valid = common_voice_valid.map(speech_file_to_array_fn, remove_columns=common_voice_valid.column_names)\n",
    "common_voice_valid = common_voice_valid.map(prepare_dataset, remove_columns=common_voice_valid.column_names)\n",
    "common_voice_valid.save_to_disk(f\"/media/monlamai/SSD/wav2vec2/valid_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)\n",
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names)\n",
    "common_voice_test.save_to_disk(f\"/media/monlamai/SSD/wav2vec2/test_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "train_arr = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(i)\n",
    "    train_batch_i = load_from_disk(f'/media/monlamai/SSD/wav2vec2/train_prepare_dataset_batch_{i}.arrow')\n",
    "    train_arr.append(train_batch_i)\n",
    "\n",
    "common_voice_train = concatenate_datasets(train_arr)\n",
    "\n",
    "# common_voice_train.save_to_disk(\"/media/monlamai/HD_volume_1/wav2vec2/train_prepare_dataset.arrow\")\n",
    "common_voice_train.save_to_disk(\"/media/monlamai/SSD/wav2vec2/train_prepare_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mv HD_volume_1/wav2vec2/train_prepare_dataset.arrow SSD/wav2vec2\n",
    "# aws s3 cp valid_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/valid_prepare_dataset.arrow --recursive\n",
    "# aws s3 cp test_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/test_prepare_dataset.arrow --recursive\n",
    "# aws s3 cp train_prepare_dataset.arrow s3://monlam.ai.stt/dataset/wav2vec2/train_prepare_dataset.arrow --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push best model to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"/media/monlamai/SSD/mms_300/mms_300_v1/checkpoint-1350000\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mms_300_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(    \"mms_300_v1.1350\")\n",
    "processor.push_to_hub(\"mms_300_v1.1350\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
